<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Logistic Regression | Applied Medical Data Science</title>
  <meta name="description" content="A compendium on Applied Medical Data Science (AMDS) with a focus on Bayesian methods." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Logistic Regression | Applied Medical Data Science" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A compendium on Applied Medical Data Science (AMDS) with a focus on Bayesian methods." />
  <meta name="github-repo" content="georgrspinner/amds" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Logistic Regression | Applied Medical Data Science" />
  
  <meta name="twitter:description" content="A compendium on Applied Medical Data Science (AMDS) with a focus on Bayesian methods." />
  

<meta name="author" content="Dr. sc. ETH Dr. med Georg Ralph Spinner" />


<meta name="date" content="2024-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="basics-of-probability.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Medical Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="basic-concepts-of-descriptive-statistics.html"><a href="basic-concepts-of-descriptive-statistics.html"><i class="fa fa-check"></i><b>2</b> Basic Concepts of Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basic-concepts-of-descriptive-statistics.html"><a href="basic-concepts-of-descriptive-statistics.html#mean-average"><i class="fa fa-check"></i><b>2.1</b> Mean (Average)</a></li>
<li class="chapter" data-level="2.2" data-path="basic-concepts-of-descriptive-statistics.html"><a href="basic-concepts-of-descriptive-statistics.html#standard-deviation"><i class="fa fa-check"></i><b>2.2</b> Standard Deviation</a></li>
<li class="chapter" data-level="2.3" data-path="basic-concepts-of-descriptive-statistics.html"><a href="basic-concepts-of-descriptive-statistics.html#correlation"><i class="fa fa-check"></i><b>2.3</b> Correlation</a></li>
<li class="chapter" data-level="2.4" data-path="basic-concepts-of-descriptive-statistics.html"><a href="basic-concepts-of-descriptive-statistics.html#root-mean-square-error-rmse"><i class="fa fa-check"></i><b>2.4</b> Root Mean Square Error (RMSE)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#univariate-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Univariate Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#variables"><i class="fa fa-check"></i><b>3.1.1</b> Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#variables-1"><i class="fa fa-check"></i><b>3.2.1</b> Variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#model-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Model Estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#interpretation"><i class="fa fa-check"></i><b>3.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regression.html"><a href="linear-regression.html#assumptions"><i class="fa fa-check"></i><b>3.2.4</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regression.html"><a href="linear-regression.html#goodness-of-fit-r-squared-r²"><i class="fa fa-check"></i><b>3.2.5</b> Goodness-of-Fit: R-squared (R²)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-model"><i class="fa fa-check"></i><b>4.1</b> Logistic Regression Model</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#univariate-logistic-regression"><i class="fa fa-check"></i><b>4.1.1</b> Univariate Logistic Regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#multivariate-logistic-regression"><i class="fa fa-check"></i><b>4.1.2</b> Multivariate Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#meaning-of-coefficients"><i class="fa fa-check"></i><b>4.2</b> Meaning of Coefficients</a></li>
<li class="chapter" data-level="4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#roc-and-auc-in-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> ROC and AUC in Logistic Regression</a></li>
<li class="chapter" data-level="4.4" data-path="logistic-regression.html"><a href="logistic-regression.html#bootstrapping"><i class="fa fa-check"></i><b>4.4</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#resampling-in-bootstrapping"><i class="fa fa-check"></i><b>4.4.1</b> Resampling in Bootstrapping</a></li>
<li class="chapter" data-level="4.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#use-in-internal-validation"><i class="fa fa-check"></i><b>4.4.2</b> Use in Internal Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="logistic-regression.html"><a href="logistic-regression.html#nagelkerkes-r²"><i class="fa fa-check"></i><b>4.5</b> Nagelkerke’s R²</a></li>
<li class="chapter" data-level="4.6" data-path="logistic-regression.html"><a href="logistic-regression.html#external-validation-using-hold-out-data"><i class="fa fa-check"></i><b>4.6</b> External Validation Using Hold-Out Data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="basics-of-probability.html"><a href="basics-of-probability.html"><i class="fa fa-check"></i><b>5</b> Basics of Probability</a>
<ul>
<li class="chapter" data-level="5.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#random-variable"><i class="fa fa-check"></i><b>5.1</b> Random Variable</a></li>
<li class="chapter" data-level="5.2" data-path="basics-of-probability.html"><a href="basics-of-probability.html#events-and-outcomes"><i class="fa fa-check"></i><b>5.2</b> Events and Outcomes</a></li>
<li class="chapter" data-level="5.3" data-path="basics-of-probability.html"><a href="basics-of-probability.html#probability"><i class="fa fa-check"></i><b>5.3</b> Probability</a></li>
<li class="chapter" data-level="5.4" data-path="basics-of-probability.html"><a href="basics-of-probability.html#independence"><i class="fa fa-check"></i><b>5.4</b> Independence</a></li>
<li class="chapter" data-level="5.5" data-path="basics-of-probability.html"><a href="basics-of-probability.html#joint-probability"><i class="fa fa-check"></i><b>5.5</b> Joint Probability</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#example-analyzing-survey-data"><i class="fa fa-check"></i><b>5.5.1</b> Example: Analyzing Survey Data</a></li>
<li class="chapter" data-level="5.5.2" data-path="basics-of-probability.html"><a href="basics-of-probability.html#example-joint-probability-from-survey-data"><i class="fa fa-check"></i><b>5.5.2</b> Example: Joint Probability from Survey Data</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="basics-of-probability.html"><a href="basics-of-probability.html#marginal-probability"><i class="fa fa-check"></i><b>5.6</b> Marginal Probability</a></li>
<li class="chapter" data-level="5.7" data-path="basics-of-probability.html"><a href="basics-of-probability.html#conditional-probability"><i class="fa fa-check"></i><b>5.7</b> Conditional Probability</a></li>
<li class="chapter" data-level="5.8" data-path="basics-of-probability.html"><a href="basics-of-probability.html#chain-rule-of-probability"><i class="fa fa-check"></i><b>5.8</b> Chain Rule of Probability</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#two-events-a-and-b"><i class="fa fa-check"></i><b>5.8.1</b> Two Events: A and B</a></li>
<li class="chapter" data-level="5.8.2" data-path="basics-of-probability.html"><a href="basics-of-probability.html#three-events-a-b-and-c"><i class="fa fa-check"></i><b>5.8.2</b> Three Events: A, B and C</a></li>
<li class="chapter" data-level="5.8.3" data-path="basics-of-probability.html"><a href="basics-of-probability.html#general-case-n-events"><i class="fa fa-check"></i><b>5.8.3</b> General Case: n Events</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="basics-of-probability.html"><a href="basics-of-probability.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>5.9</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#example-calculating-total-probability"><i class="fa fa-check"></i><b>5.9.1</b> Example: Calculating Total Probability</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="basics-of-probability.html"><a href="basics-of-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>5.10</b> Bayes’ Theorem</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#example-medical-diagnosis"><i class="fa fa-check"></i><b>5.10.1</b> Example: Medical Diagnosis</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="basics-of-probability.html"><a href="basics-of-probability.html#calculating-the-positive-predictive-value-ppv"><i class="fa fa-check"></i><b>5.11</b> Calculating the Positive Predictive Value (PPV)</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#example-calculating-ppv-with-known-sensitivity-and-specificity"><i class="fa fa-check"></i><b>5.11.1</b> Example: Calculating PPV with Known Sensitivity and Specificity</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="basics-of-probability.html"><a href="basics-of-probability.html#probability-distributions"><i class="fa fa-check"></i><b>5.12</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>5.12.1</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="5.12.2" data-path="basics-of-probability.html"><a href="basics-of-probability.html#continuous-probability-distributions"><i class="fa fa-check"></i><b>5.12.2</b> Continuous Probability Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="basics-of-probability.html"><a href="basics-of-probability.html#expectation-in-probability"><i class="fa fa-check"></i><b>5.13</b> Expectation in Probability</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="basics-of-probability.html"><a href="basics-of-probability.html#discrete-random-variable"><i class="fa fa-check"></i><b>5.13.1</b> Discrete Random Variable</a></li>
<li class="chapter" data-level="5.13.2" data-path="basics-of-probability.html"><a href="basics-of-probability.html#continuous-random-variable"><i class="fa fa-check"></i><b>5.13.2</b> Continuous Random Variable</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html"><i class="fa fa-check"></i><b>6</b> Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#likelihood"><i class="fa fa-check"></i><b>6.1</b> Likelihood</a></li>
<li class="chapter" data-level="6.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>6.2</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="6.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#prior"><i class="fa fa-check"></i><b>6.3</b> Prior</a></li>
<li class="chapter" data-level="6.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#posterior"><i class="fa fa-check"></i><b>6.4</b> Posterior</a></li>
<li class="chapter" data-level="6.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-inference"><i class="fa fa-check"></i><b>6.5</b> Bayesian inference</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#analytically"><i class="fa fa-check"></i><b>6.5.1</b> Analytically</a></li>
<li class="chapter" data-level="6.5.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#maximum-a-posteriori-map-estimate"><i class="fa fa-check"></i><b>6.5.2</b> Maximum-a-posteriori (MAP) estimate</a></li>
<li class="chapter" data-level="6.5.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#markov-chain-monte-carlo-mcmc-sampling"><i class="fa fa-check"></i><b>6.5.3</b> Markov chain Monte Carlo (MCMC) sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-networks.html"><a href="bayesian-networks.html"><i class="fa fa-check"></i><b>7</b> Bayesian Networks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-networks.html"><a href="bayesian-networks.html#mathematical-representation"><i class="fa fa-check"></i><b>7.1</b> Mathematical Representation</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-networks.html"><a href="bayesian-networks.html#graphical-representation"><i class="fa fa-check"></i><b>7.2</b> Graphical representation</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-networks.html"><a href="bayesian-networks.html#discrete-bayesian-networks"><i class="fa fa-check"></i><b>7.3</b> Discrete Bayesian Networks</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-networks.html"><a href="bayesian-networks.html#conditional-probability-tables"><i class="fa fa-check"></i><b>7.3.1</b> Conditional Probability Tables</a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian-networks.html"><a href="bayesian-networks.html#example-cpt-of-weather-and-picnic"><i class="fa fa-check"></i><b>7.3.2</b> Example: CPT of Weather and Picnic</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian-networks.html"><a href="bayesian-networks.html#example-discrete-bn-of-sprinkler-wet-grass-and-rain"><i class="fa fa-check"></i><b>7.3.3</b> Example: Discrete BN of sprinkler, wet grass and rain</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-networks.html"><a href="bayesian-networks.html#continuous-gaussian-bayesian-networks"><i class="fa fa-check"></i><b>7.4</b> Continuous (Gaussian) Bayesian Networks</a></li>
<li class="chapter" data-level="7.5" data-path="bayesian-networks.html"><a href="bayesian-networks.html#conditional-gaussian-bayesian-networks-cgbns"><i class="fa fa-check"></i><b>7.5</b> Conditional Gaussian Bayesian Networks (CGBNs)</a></li>
<li class="chapter" data-level="7.6" data-path="bayesian-networks.html"><a href="bayesian-networks.html#structure-learning"><i class="fa fa-check"></i><b>7.6</b> Structure Learning</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian-networks.html"><a href="bayesian-networks.html#a-priori-knowledge"><i class="fa fa-check"></i><b>7.6.1</b> A priori knowledge</a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian-networks.html"><a href="bayesian-networks.html#score-based-methods"><i class="fa fa-check"></i><b>7.6.2</b> <strong>Score-Based Methods</strong></a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian-networks.html"><a href="bayesian-networks.html#constraint-based-methods"><i class="fa fa-check"></i><b>7.6.3</b> Constraint-Based Methods</a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian-networks.html"><a href="bayesian-networks.html#definition"><i class="fa fa-check"></i><b>7.6.4</b> Definition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Medical Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Logistic Regression<a href="logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Logistic regression is used for binary classification problems—predicting outcomes that have two possible values. It is hence particularly useful for binary classification tasks.</p>
<p>Logistic regression is a powerful tool for classification problems, especially in fields like medical data science and social sciences.</p>
<p>The extension for discrete (categorical) outcomes (i.e. more than two) is called multinomial logistic regression.</p>
<p><a href="https://en.wikipedia.org/wiki/File:Exam_pass_logistic_curve.svg"><img src="images/Exam_pass_logistic_curve.svg.png" alt="Logistic regression plot example: data-points (black) and fitted logistic regression curve (blue). Source: https://en.wikipedia.org/wiki/File:Exam_pass_logistic_curve.svg" /></a></p>
<div id="logistic-regression-model" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Logistic Regression Model<a href="logistic-regression.html#logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In logistic regression, the probability of the target variable being true is modeled as a function of the independent variables. The logistic function transforms linear regression output into probabilities:</p>
<p><span class="math display">\[ \log\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n \]</span></p>
<p>where <span class="math inline">\(p\)</span> is the probability of the dependent variable equaling a case (often denoted as 1). Note that <span class="math inline">\(p\)</span> itself is a value between 0 and 1.</p>
<p>Logistic regression hence models the log-odds <span class="math inline">\(\frac{p}{1 - p}\)</span> of the probability <span class="math inline">\(p\)</span> of an event occurring.</p>
<div id="univariate-logistic-regression" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Univariate Logistic Regression<a href="logistic-regression.html#univariate-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Involves a single independent variable <span class="math inline">\(x\)</span>. Univariate logistic regression models the relationship between a predictor variable and the probability of a particular outcome. The model is expressed as:</p>
<p><span class="math display">\[ \log\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1x \]</span></p>
<ul>
<li><span class="math inline">\(p\)</span> is the probability of the event (e.g., having a disease).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1\)</span> is the coefficient for the predictor variable.</li>
<li><span class="math inline">\(x\)</span> is the predictor variable (e.g., blood pressure).</li>
</ul>
</div>
<div id="multivariate-logistic-regression" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Multivariate Logistic Regression<a href="logistic-regression.html#multivariate-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Involves not olny one predictor <span class="math inline">\(x\)</span>, but multiple independent variables <span class="math inline">\(x_1, x_2, …\)</span></p>
</div>
</div>
<div id="meaning-of-coefficients" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Meaning of Coefficients<a href="logistic-regression.html#meaning-of-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Each coefficient in the model represents the change in the log-odds of the dependent variable per unit change in the predictor. For a positive coefficient, as the predictor increases, the log-odds of the outcome occurring increase, and thus the probability of the outcome occurring also increases.</p>
<div style="color: gray;">
<p><strong>Example: Impact of Coefficients in Logistic Regression by Doubling of Odds</strong></p>
<p>To illustrate how a coefficient in logistic regression influences the prediction, let’s consider an example. Assume we have a univariate logistic regression model where the outcome variable is whether a patient has a certain disease (yes or no), and we have one predictor variable, say, blood pressure.</p>
<p>Consider again therefore: <span class="math display">\[ \log\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1x \]</span></p>
<p>If <span class="math inline">\(\beta_1 = \log(2)\)</span> or approximately 0.693, this indicates that for each one-unit increase in <span class="math inline">\(x\)</span>, the odds of the event occurring are doubled. This is derived from:</p>
<p><span class="math display">\[ \log\left(\frac{p}{1 - p}\right) = \beta_0 + \log(2) \cdot x \]</span></p>
<p>In order to calculate the odds, the exponential function is used on both sides of the equation. When <span class="math inline">\(x\)</span> increases by 1, the log-odds increase by <span class="math inline">\(\log(2)\)</span>, and the odds are thus multiplied by <span class="math inline">\(e^{\log(2)} = 2\)</span>. Thus, the probability of the event occurring becomes twice as high for each unit increase in the predictor variable.</p>
</div>
</div>
<div id="roc-and-auc-in-logistic-regression" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> ROC and AUC in Logistic Regression<a href="logistic-regression.html#roc-and-auc-in-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#/media/File:Roccurves.png"><img src="images/Roccurves.png" alt="Receiver-operator-characteristic (ROC) curve example: three different models are compared. Soruce: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#/media/File:Roccurves.png" /></a></p>
<p>The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classification model’s diagnostic ability. ROC curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) for different threshold values.</p>
<p>More specifically, in logistic regression, ROC curve is created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The TPR is the proportion of positive cases correctly identified, while the FPR is the proportion of negative cases incorrectly identified as positive.</p>
<p>To calculate the ROC curve, you typically:</p>
<ol style="list-style-type: decimal">
<li><p>Predict the probability <span class="math inline">\(p\)</span> of the positive class using the logistic regression model for each instance in the dataset.</p></li>
<li><p>Vary the decision threshold from 0 to 1, i.e. check whether <span class="math inline">\(p\)</span> is larger said threshold.</p></li>
<li><p>At each threshold, calculate the TPR and FPR, i.e. check how many cases are correctly identified as such and which are misclassified.</p></li>
<li><p>Plot these values, creating a curve that shows the trade-off between TPR and FPR at different thresholds.</p></li>
</ol>
<p>Area Under the Curve (AUC) provides a single measure of a model’s performance, quantifying the overall ability of the test to discriminate between positive and negative cases. The AUC can theoretically be 0, but a random classifier would achieve a AUC of 0.5 The maximum (and hence ideal) AUC is 1.</p>
</div>
<div id="bootstrapping" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Bootstrapping<a href="logistic-regression.html#bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bootstrapping is a resampling technique used to estimate the distribution of a statistic (like the mean or median) by repeatedly sampling with replacement from the data set. In logistic regression, it’s often used for internal validation, to assess the stability and reliability of the model.</p>
<div id="resampling-in-bootstrapping" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Resampling in Bootstrapping<a href="logistic-regression.html#resampling-in-bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Resampling in bootstrapping involves repeatedly drawing samples (with replacement, i.e. the same data-point can be selected several times) from the original dataset and calculating the statistic of interest for each sample. This process creates a distribution of the statistic, allowing for estimation of its variance and confidence intervals.</p>
<div style="color: gray;">
<p><strong>Example: Bootstrapping</strong></p>
<p>Imagine a dataset with 100 observations. In bootstrapping, you might randomly select 100 observations from this dataset, with replacement (i.e. there are duplicates), to form a new sample. This process is repeated many times (e.g., 1000 times) to create many bootstrap samples.</p>
</div>
</div>
<div id="use-in-internal-validation" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Use in Internal Validation<a href="logistic-regression.html#use-in-internal-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bootstrapping is useful for internal validation of models, such as logistic regression models, in several ways:</p>
<ul>
<li><p>Stability Testing: It helps in assessing the stability of the model by examining how much the predictions vary across different bootstrap samples.</p></li>
<li><p>Error Estimation: Bootstrapping can be used to estimate the error (like standard error) of model coefficients or predictions.</p></li>
<li><p>Overfitting Assessment: It helps in assessing whether a model is overfitting the training data by evaluating its performance across multiple resampled datasets.</p></li>
</ul>
<p>This method is particularly valuable when the original dataset is not large enough to split into separate training and validation sets, as it allows for effective internal validation using the available data.</p>
<p>An alternative method would be cross-validation, where the data is repeatedly split up into training and test batches.</p>
</div>
</div>
<div id="nagelkerkes-r²" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Nagelkerke’s R²<a href="logistic-regression.html#nagelkerkes-r²" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This is a pseudo R-squared measure for logistic regression.</p>
<p>Nagelkerke’s R² is a modified version of the R-squared statistic adapted for logistic regression. It provides an estimation of the variance explained by the model.</p>
<p><span class="math display">\[ R_{\text{Nagelkerke}}^2 = \frac{1 - e^{-2/n(L_{\text{model}} - L_0)}}{1 - e^{-2/n(L_{\text{max}} - L_0)}} \]</span></p>
<p>where <span class="math inline">\(L_{\text{model}}\)</span> is the log-likelihood of the model, <span class="math inline">\(L_0\)</span> is the log-likelihood of the null model, <span class="math inline">\(L_{\text{max}}\)</span> is the log-likelihood of the perfect model, and <span class="math inline">\(n\)</span> is the sample size.</p>
<p>Nagelkerke’s R² is an adaptation of the Cox &amp; Snell R² that adjusts the scale so the maximum value is 1.</p>
</div>
<div id="external-validation-using-hold-out-data" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> External Validation Using Hold-Out Data<a href="logistic-regression.html#external-validation-using-hold-out-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>External validation involves assessing the model’s predictive performance on a separate dataset that was not used during the model building process. This process helps to evaluate how well the model generalizes to new, unseen data, which is crucial for assessing the practical utility of the model.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="basics-of-probability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cjvanlissa/gitbook-demo/edit/master/logistic-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
